utils::globalVariables(c(":=", "in_bin", "bin_id"))

#' Conditional density estimation with HAL in a single cross-validation fold
#'
#' @param fold Object specifying cross-validation folds as generated by a call
#'  to \code{origami::make_folds}.
#' @param long_data A \code{data.table} or \code{data.frame} object containing
#'  the data in long format, as given in \insertRef{diaz2011super}{haldensify},
#'  as produced by \code{\link{format_long_hazards}}.
#' @param wts A \code{numeric} vector of observation-level weights, matching in
#'  its length the number of records present in the long format data. The
#'  default is to weight all observations equally.
#' @param lambda_seq A \code{numeric} sequence of values of the lambda tuning
#'  parameter of the Lasso L1 regression, to be passed to \code{glmnet::glmnet}
#'  through a call to \code{hal9001::fit_hal}.
#'
#' @importFrom stats aggregate plogis
#' @importFrom origami training validation fold_index
#' @importFrom future.apply future_lapply
#' @importFrom assertthat assert_that
#' @importFrom hal9001 fit_hal
#
cv_haldensify <- function(fold, long_data, wts = rep(1, nrow(long_data)),
                          lambda_seq = exp(seq(-1, -13, length = 1000))) {
  # make training and validation folds
  train_set <- origami::training(long_data)
  valid_set <- origami::validation(long_data)

  # subset observation-level weights to the correct size
  wts_train <- wts[fold$training_set]
  wts_valid <- wts[fold$validation_set]

  # fit a HAL regression on the training set
  # NOTE: pass IDs to glmnet?
  hal_fit_train <- hal9001::fit_hal(
    X = as.matrix(train_set[, -c(1, 2)]),
    Y = as.numeric(train_set$in_bin),
    fit_type = "glmnet",
    use_min = TRUE,
    family = "binomial",
    return_lasso = TRUE,
    lambda = lambda_seq,
    fit_glmnet = TRUE,
    standardize = FALSE, # pass to glmnet
    weights = wts_train, # pass to glmnet
    yolo = FALSE
  )

  # get intercept and coefficient fits for this value of lambda from glmnet
  alpha_hat <- hal_fit_train$glmnet_lasso$a0
  betas_hat <- hal_fit_train$glmnet_lasso$beta
  coefs_hat <- rbind(alpha_hat, betas_hat)

  # make design matrix for validation set manually
  pred_x_basis <- hal9001:::make_design_matrix(
    as.matrix(valid_set[, -c(1, 2)]),
    hal_fit_train$basis_list
  )
  pred_x_basis <- hal9001:::apply_copy_map(
    pred_x_basis,
    hal_fit_train$copy_map
  )
  pred_x_basis <- cbind(rep(1, nrow(valid_set)), pred_x_basis)

  # manually predict along sequence of lambdas
  preds_logit <- pred_x_basis %*% coefs_hat
  preds <- stats::plogis(as.matrix(preds_logit))

  # compute hazard for a given observation by looping over individuals
  density_pred_each_obs <-
    future.apply::future_lapply(unique(valid_set$obs_id), function(id) {
      # get predictions for the current observation only
      hazard_pred_this_obs <- matrix(preds[valid_set$obs_id == id, ],
        ncol = length(lambda_seq)
      )

      # map hazard to density for a single observation and return
      density_pred_this_obs <-
        map_hazard_to_density(hazard_pred_single_obs = hazard_pred_this_obs)

      return(density_pred_this_obs)
    })

  # aggregate predictions across observations
  density_pred <- do.call(rbind, density_pred_each_obs)

  # collapse weights to the observation level
  wts_valid_reduced <- stats::aggregate(
    wts_valid, list(valid_set$obs_id),
    unique
  )
  colnames(wts_valid_reduced) <- c("id", "weight")

  # construct output
  out <- list(
    preds = density_pred,
    ids = wts_valid_reduced$id,
    wts = wts_valid_reduced$weight,
    fold = origami::fold_index()
  )
  return(out)
}

################################################################################

#' Cross-validated conditional density estimation with HAL
#'
#' @param A The \code{numeric} vector or similar of the observed values of an
#'  intervention for a group of observational units of interest.
#' @param W A \code{data.frame}, \code{matrix}, or similar giving the values of
#'  baseline covariates (potential confounders) for the observed units whose
#'  observed intervention values are provided in the previous argument.
#' @param wts A \code{numeric} vector of observation-level weights. The default
#'  is to weight all observations equally.
#' @param grid_type A \code{character} indicating the strategy to be used in
#'  creating bins along the observed support of the intervention \code{A}. For
#'  bins of equal range, use "equal_range", consulting the documentation of
#'  \code{ggplot2::cut_interval} for more information. To ensure each bins has
#'  the same number of points, use "equal_mass" and consult the documentation of
#'  \code{ggplot2::cut_number} for details.
#' @param n_bins Only used if \code{type} is set to \code{"equal_range"} or
#'  \code{"equal_mass"}. This \code{numeric} value indicates the number of bins
#'  that the support of the intervention \code{A} is to be divided into.
#' @param lambda_seq A \code{numeric} sequence of values of the lambda tuning
#'  parameter of the Lasso L1 regression, to be passed to \code{glmnet::glmnet}
#'  through a call to \code{hal9001::fit_hal}.
#'
#' @importFrom origami make_folds cross_validate
#' @importFrom hal9001 fit_hal
#'
#' @export
#
haldensify <- function(A, W, wts = rep(1, length(A)),
                       grid_type = c(
                         "equal_range", "equal_mass"
                       ),
                       n_bins = 10,
                       lambda_seq = exp(seq(-1, -13, length = 1000))) {
  # catch input
  call <- match.call(expand.dots = TRUE)

  # re-format input data into long hazards structure
  reformatted_output <- format_long_hazards(
    A = A, W = W, wts = wts,
    type = grid_type, n_bins = n_bins
  )
  long_data <- reformatted_output$data
  breakpoints <- reformatted_output$breaks
  bin_sizes <- reformatted_output$bin_length

  # extract weights from long format data structure
  wts_long <- long_data$wts
  long_data[, wts := NULL]

  # make folds with origami
  folds <- origami::make_folds(long_data, cluster_ids = long_data$obs_id)

  # call cross_validate on cv_density function...
  haldensity <- origami::cross_validate(
    cv_fun = cv_haldensify,
    folds = folds,
    long_data = long_data,
    wts = wts_long,
    lambda_seq = lambda_seq,
    use_future = FALSE,
    .combine = FALSE
  )

  # re-organize output cross-validation procedure
  density_pred_unscaled <- do.call(rbind, haldensity$preds)
  # re-scale predictions by multiplying by bin width for bin each fails in
  density_pred_scaled <- apply(density_pred_unscaled, 2, function(x) {
    pred <- x / bin_sizes[long_data[in_bin == 1, bin_id]]
    return(pred)
  })
  obs_wts <- do.call(c, haldensity$wts)

  # compute loss for the given individual
  density_loss <- apply(density_pred_scaled, 2, function(x) {
    pred_weighted <- x * obs_wts
    loss_weighted <- -log(pred_weighted)
    return(loss_weighted)
  })

  # take column means to have average loss across sequence of lambdas
  loss_mean <- colMeans(density_loss)
  lambda_loss_min <- lambda_seq[which.min(loss_mean)]

  # fit a HAL regression on the full data set with the CV-selected lambda
  hal_fit <- hal9001::fit_hal(
    X = as.matrix(long_data[, -c(1, 2)]),
    Y = as.numeric(long_data$in_bin),
    fit_type = "glmnet",
    use_min = TRUE,
    family = "binomial",
    return_lasso = TRUE,
    lambda = lambda_loss_min,
    fit_glmnet = TRUE,
    standardize = FALSE,  # pass to glmnet
    weights = wts_long,   # pass to glmnet
    yolo = FALSE
  )

  # construct output
  out <- list(
    hal_fit = hal_fit,
    breaks = breakpoints,
    bin_sizes = bin_sizes,
    call = call
  )
  class(out) <- "haldensify"
  return(out)
}

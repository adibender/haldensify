---
title: "Highly Adaptive Lasso Conditional Density Estimation"
author: "[Nima Hejazi](https://nimahejazi.org) and [David
  Benkeser](https://www.sph.emory.edu/faculty/profile/#!dbenkes)"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
bibliography: ../inst/REFERENCES.bib
vignette: >
  %\VignetteIndexEntry{Highly adaptive lasso conditional density estimation}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

# Background and Motivations

In causal inference problems, both classical estimators (e.g., based on inverse
probability weighting) and doubly robust estimators (e.g., one-step estimation,
targeted minimum loss estimation) require estimation of the propensity score, a
nuisance parameter corresponding to the treatment mechanism. While exposures of
interest may often be continuous-valued, most approaches opt to discretize the
exposure so as to estimate effects based on categorical exposures -- such a
simplification is often done out of convenience, to avoid estimation of the
_generalized propensity score_, which is a conditional density function. The
`haldensify` package

In the
following, we consider 

A simple strategy for estimating the generalized propensity score $q_{0,A}$
is to assume a parametric working model and use parametric regression to
generate suitable density estimates. Unfortunately, most such approaches do not
allow for flexible modeling of $q_{0,A}$.
%For example, one could operate
%under the working assumption that $A$ given $W$ follows a Gaussian distribution
%with homoscedastic variance and mean $\sum_{j=1}^p \beta_j \phi_j(W)$, where
%$\phi = (\phi_j : j)$ are user-selected basis functions and $\beta = (\beta_j
%: j)$ are unknown regression parameters. In this case, a density estimate would
%be generated by fitting a linear regression of $A$ on $\phi(W)$
%(e.g., minimizing inverse probability of sampling weighted least squares)
%to estimate the conditional mean of $A$ given $W$, paired with an estimate of
%the variance of $A$. Then, the estimated conditional density would be given by
%the density of a Gaussian distribution evaluated at these estimates.
The relative dearth of available estimators of a conditional density motivated
our development of a novel estimator that accounts for two-phase sampling
designs. We detail this approach in the \href{sm}{Supplementary Materials}
and provide an implementation of our proposal in the \texttt{haldensify}
\texttt{R} package~\citep{hejazi2020haldensify}. Going forward, we denote by
$q_{n,A}(a \mid w)$ the estimated conditional density of $A$ given $W = w$,
evaluated at $a \in \mathcal{A}$.

# Pooled hazards conditional density estimation

In section~\ref{main:est_nuisance_param}, several of the challenges associated
with conditional density estimation were briefly expressed. The auxiliary
covariate given in equation (\ref{main:eqn:aux_covar}) of the EIF is a ratio of
such conditional densities. In the construction of our proposed estimators, the
exposure mechanism $q_{0,A}$ is a density of the intervention $A$, conditional
on the observed covariates $W$, which must be evaluated at both the observed
values and the post-intervention counterfactual values. As consistent estimation
of the the generalized propensity score is an integral part of our proposed
methodology, we outline here a conditional density estimator, built around the
HAL regression function, that achieves the convergence rate required by our
formal theorem. We note that proposals for the data adaptive estimation of such
quantities are sparse in the literature~\citep[e.g.,][]{zhu2015boosting}.
Notably, \citet{diaz2011super} gave a proposal for constructing semiparametric
estimators of such a target quantity based on exploiting the relationship
between the hazard and density functions. Our proposal builds upon theirs in
several key ways: (i) we adjust their algorithm so as to incorporate
sample-level weights, necessary for making use of inverse probability of
censoring weights; and (ii) we replace their use of an arbitrary classification
model with one based on the HAL regression function. While our first
modification is general and may be applied to the estimation strategy
\citet{diaz2011super} propose, our latter contribution requires adjusting the
penalization aspect of HAL regression to respect the use of a loss function
appropriate for prediction on the hazard scale. To make these contributions
widely accessible, we introduce the \texttt{haldensify} \texttt{R}
package~\citep{hejazi2020haldensify}, available at
\url{https://github.com/nhejazi/haldensify}

To build an estimator of a conditional density, \citet{diaz2011super} considered
discretizing the observed $a \in A$ based on a number of bins $T$ and a binning
procedure (e.g., including the same number of points in each bin or forcing
bins to be of the same length). We note that the choice of the tuning parameter
$T$ corresponds roughly to the choice of bandwidth in classical kernel density
estimation; this will be made clear upon further examination of the proposed
algorithm. The data $\{A, W\}$ are reformatted such that the hazard of an
observed value $a \in A$ falling in a given bin may be evaluated via standard
classification techniques. In fact, this proposal may be viewed as
a re-formulation of the classification problem into a corresponding set of
hazard regressions:
\begin{align*}
   \prob (a \in [\alpha_{t-1}, \alpha_t) \mid W) =& \prob (a \in [\alpha_{t-1},
   \alpha_t) \mid A \geq \alpha_{t-1}, W) \times  \\ & \prod_{j = 1}^{t -1} \{1
   - \prob (a \in [\alpha_{j-1}, \alpha_j) \mid A
   \geq \alpha_{j-1}, W) \},
\end{align*}
where the probability that a value of $a \in A$ falls in a bin $[\alpha_{t-1},
\alpha_t)$ may be directly estimated from a standard classification model. The
likelihood of this model may be re-expressed in terms of the likelihood of
a binary variable in a data set expressed through a repeated measures structure.
Specifically, this re-formatting procedure is carried out by creating a data set
in which any given observation $A_i$ appears (repeatedly) for as many intervals
$[\alpha_{t-1}, \alpha_t)$ that there are prior to the interval to which the
observed $a$ belongs. A new binary outcome variable, indicating $A_i \in
[\alpha_{t-1}, \alpha_t)$, is recorded as part of this new data structure. With
the re-formatted data, a pooled hazard regression, spanning the support of $A$
is then executed. Finally, the conditional density estimator
\begin{equation*}
   q_{n, \alpha}(a \mid W) = \frac{\prob(a \in [\alpha_{t-1}, \alpha_t)
      \mid W)}{(\alpha_t - \alpha_{t-1})},
\end{equation*}
for $\alpha_{t-1} \leq a \le \alpha_t$, may be constructed. As part of this
procedure, the hazard estimates are mapped to density estimates through
rescaling of the estimates by the bin size ($\alpha_t - \alpha_{t-1}$).

In its original proposal, a key element of this procedure was the use of any
arbitrary classification procedure for estimating $\prob(a \in [\alpha_{t-1},
\alpha_t) \mid W)$, facilitating the incorporation of flexible, data adaptive
estimators. We alter this proposal in two ways, (i) replacing the arbitrary
estimator of $\prob(a \in [\alpha_{t-1}, \alpha_t) \mid W)$ with HAL regression
and (ii) accommodating the use of sample-level weights, making it possible for
the resultant conditional density estimator to achieve a convergence rate with
respect to a loss-based dissimilarity of $n^{-1/4}$ under only mild assumptions.
This is an important advance that is needed for the asymptotic analysis of our
proposed estimators, as per section~\ref{main:asymp_analy}. As a secondary
advance, our procedure alters the HAL regression function to use a loss
function
tailored for estimation of the hazard, invoking $\ell_1$-penalization in
a manner consistent with this loss.

# Problem Setup and Notation

The problem addressed by the work of @diaz2020nonparametric may be represented
by the following nonparametric structural equation model (NPSEM):
\begin{align*}
  W &= f_W(U_W); A = f_A(W, U_A); Z=f_Z(W, A, U_Z);\\ \nonumber
  M &= f_M(W, A, Z, U_M); Y = f_Y(W, A, Z, M, U_Y).
\end{align*}
In the NPSEM, $W$ denotes a vector of observed pre-treatment covariates, $A$
denotes a categorical treatment variable, $Z$ denotes an intermediate confounder
affected by treatment, $M$ denotes a (possibly multivariate) mediator, and $Y$
denotes a continuous or binary outcome.  The vector of exogenous factors
$U=(U_W,U_A,U_Z,U_M,U_Y)$, and the functions $f$, are assumed deterministic but
unknown.  Importantly, the NPSEM encodes a time-ordering between these variables
and allows the evaluation of counterfactual quantities defined by intervening on
a set of nodes of the NPSEM.  The observed data unit can be represented by the
random variable $O = (W, A, Z, M, Y)$; we consider access to $O_1, \ldots, O_n$,
a sample of $n$ i.i.d. observations of $O$.

@diaz2020nonparametric additionally define the following parameterizations,
familiarity with which will be useful for using the [`medoutcon` `R`
package](https://github.com/nhejazi/medoutcon). In particular, these authors
define $g(a \mid w)$ as the probability mass function of $A = a$ conditional on
$W = w$ and use $h(a \mid m, w)$ to denote the probability mass function of $A
= a$ conditional on $(M, W) = (m, w)$. Further, @diaz2020nonparametric use
$b(a, z, m, w)$ to denote the outcome regression function $\mathbb{E}(Y \mid A
= a, Z = z, M = m, W = w)$, as well as $q(z \mid a,w)$ and $r(z \mid a, m, w)$
to denote the corresponding conditional densities of $Z$.

# Data Analysis Example

## Setting up the data example

Now, we'll take a look at how to estimate the interventional direct and indirect
effects using a simulated data example. @diaz2020nonparametric illustrate the
use of their estimators of these effects in an application in which they seek to
elucidate the mechanisms behind the unintended harmful effects that a housing
intervention had on adolescent girls' risk behavior.

First, let's load a few required packages and set a seed for our simulation.

```{r setup, message=FALSE, warning=FALSE}
library(data.table)
library(medoutcon)
library(sl3)
set.seed(75681)
n_obs <- 500      # number of observations in our simulated dataset
```

Next, we'll generate a very simple simulated dataset. The function
`make_example_data`, defined below, generates three binary baseline covariates
$W = (W_1, W_2, W_3)$, a binary exposure variable $A$, a single binary mediateor
$M$ an intermediate confounder $Z$ that affects the mediator $M$ and is itself
affected by the exposure $A$, and, finally, a binary outcome $Y$ that is a
function of $(W, A, Z, M)$.

```{r make_example_data, message=FALSE, warning=FALSE}
# produces a simple data set based on ca causal model with mediation
make_example_data <- function(n_obs = 1000) {
  ## baseline covariates
  w_1 <- rbinom(n_obs, 1, prob = 0.6)
  w_2 <- rbinom(n_obs, 1, prob = 0.3)
  w_3 <- rbinom(n_obs, 1, prob = pmin(0.2 + (w_1 + w_2) / 3, 1))
  w <- cbind(w_1, w_2, w_3)
  w_names <- paste("W", seq_len(ncol(w)), sep = "_")

  ## exposure
  a <- as.numeric(rbinom(n_obs, 1, plogis(rowSums(w) - 2)))

  ## mediator-outcome confounder affected by treatment
  z <- rbinom(n_obs, 1, plogis(rowMeans(-log(2) + w - a) + 0.2))

  ## mediator -- could be multivariate
  m <- rbinom(n_obs, 1, plogis(rowSums(log(3) * w[, -3] + a - z)))
  m_names <- "M"

  ## outcome
  y <- rbinom(n_obs, 1, plogis(1 / (rowSums(w) - z + a + m)))

  ## construct output
  dat <- as.data.table(cbind(w = w, a = a, z = z, m = m, y = y))
  setnames(dat, c(w_names, "A", "Z", m_names, "Y"))
  return(dat)
}

# set seed and simulate example data
example_data <- make_example_data(n_obs)
w_names <- stringr::str_subset(colnames(example_data), "W")
m_names <- stringr::str_subset(colnames(example_data), "M")
```

Now, let's take a quick look at our simulated data:

```{r example_data, message=FALSE, warning=FALSE}
# quick look at the data
head(example_data)
```

As noted above, all covariates in our dataset are binary; however, note that
this need not be the case for using our methodology --- in particular, the only
current limitation is that the intermediate confounder $Z$ must be binary when
using our implemented TML estimator of the (in)direct effects.

Using this dataset, we'll proceed to estimate the interventional (in)direct
effects. In order to do so, we'll need to estimate several nuisance parameters,
including the exposure mechanism $g(A \mid W)$, a re-parameterized exposure
mechanism that conditions on the mediators $h(A \mid M, W)$, the outcome
mechanism $b(Y \mid M, Z, A, W)$, and two variants of the intermediate
confounding mechanism $q(Z \mid A, W)$ and $r(Z \mid M, A, W)$. In order to
estimate each of these nuisance parameters flexibly, we'll rely on data adaptive
regression strategies in order to avoid the potential for (parametric) model
misspecification.

<!--
Note that there are two additional nuisance parameters that must also be
estimated ($u$ and $v$), which are themselves functions of the other nuisance
parameters.  We recommend estimating these via the highly adaptive lasso, which
is the...
-->

## Ensemble learning of nuisance functions

As we'd like to rely on flexible, data adaptive regression strategies for
estimating each of the nuisance parameters $(g, h, b, q, r)$, we require a
method for choosing among or combining the wide variety of available regression
strategies. For this, we recommend the use of the Super Learner algorithm for
ensemble machine learning [@vdl2007super].  The recently developed [`sl3` R
package](https://tlverse.org/sl3) [@coyle2020sl3] provides a unified interface
for deploying a wide variety of machine learning algorithms (simply called
_learners_ in the `sl3` nomenclature) as well as for constructing Super Learner
ensemble models of such learners. For a complete guide on using the `sl3` R
package, consider consulting https://tlverse.org/sl3, or https://tlverse.org
(and https://github.com/tlverse) for the `tlverse` ecosystem, of which `sl3` is
an integral part.

To construct an ensemble learner using a handful of popular machine learning
algorithms, we'll first instantiate variants of learners from the appropriate
classes for each algorithm, and then create a Super Learner ensemble via the
`Lrnr_sl` class. Below, we demonstrate the construction of an ensemble learner
based on a modeling library including an intercept model, a main-terms GLM,
$\ell_1$-penalized Lasso regression, $\ell_2$-penalized ridge regression, an
elastic net regression that equally weights the $\ell_1$ and $\ell_2$ penalties,
extreme gradient boosted trees (`xgboost`), and the highly adaptive lasso (HAL):

```{r make_sl, message=FALSE, warning=FALSE}
# instantiate learners
mean_lrnr <- Lrnr_mean$new()
fglm_lrnr <- Lrnr_glm_fast$new(family = binomial())
lasso_lrnr <- Lrnr_glmnet$new(alpha = 1, family = "binomial", nfolds = 3)
ridge_lrnr <- Lrnr_glmnet$new(alpha = 0, family = "binomial", nfolds = 3)
enet_lrnr <- Lrnr_glmnet$new(alpha = 0.5, family = "binomial", nfolds = 3)
xgb_lrnr <- Lrnr_xgboost$new(nrounds = 50, objective = "reg:logistic")

# for HAL, use linear probability formulation, with bounding in unit interval
hal_gaussian_lrnr <- Lrnr_hal9001$new(max_degree = 3, n_folds = 3,
                                      use_min = FALSE, type.measure = "mse",
                                      nlambda = 200, family = "gaussian",
                                      lambda.min.ratio = 1 / n_obs)
bound_lrnr <- Lrnr_bound$new(bound = 1e-6)
hal_bounded_lrnr <- Pipeline$new(hal_gaussian_lrnr, bound_lrnr)

# create learner library and instantiate super learner ensemble
lrnr_lib <- Stack$new(mean_lrnr, fglm_lrnr, enet_lrnr, lasso_lrnr, ridge_lrnr,
                      xgb_lrnr, hal_bounded_lrnr)
sl_lrnr <- Lrnr_sl$new(learners = lrnr_lib, metalearner = Lrnr_nnls$new())
```

While we recommend the use of a Super Learner ensemble model like the one
constructed above in practice, such a library will be too computationally
intensive for our examples. To reduce computation time, we construct a simpler
library, using only a subset of the above learning algorithms:

```{r make_simple_sl, message=FALSE, warning=FALSE}
# create simpler learner library and instantiate super learner ensemble
lrnr_lib <- Stack$new(mean_lrnr, fglm_lrnr, lasso_lrnr, xgb_lrnr)
sl_lrnr <- Lrnr_sl$new(learners = lrnr_lib, metalearner = Lrnr_nnls$new())
```

Having set up our ensemble learner, we're now ready to estimate each of the
interventional effects using the efficient estimators exposed in the `medoutcon`
package.

## Estimating the direct effect

We're now ready to estimate the interventional direct effect. This direct effect
is computed as a contrast between the interventions $(a' = 1, a^{\star} = 0)$
and $(a' = 0, a^{\star} = 0)$. In particular, our efficient estimators of the
interventional direct effect proceed by constructing estimators
$\hat{\theta}(a' = 1, a^{\star} = 0)$ and $\hat{\theta}(a' = 0, a^{\star} = 0)$.
Then, an efficient estimator of the direct effect is available by application
of the delta method, that is, $\hat{\theta}^{\text{DE}} =
\hat{\theta}(a' = 1, a^{\star} = 0) - \hat{\theta}(a' = 0, a^{\star} = 0)$.
Applying the same principle to the EIF estimates, one can derive variance
estimates and construct asymptotically correct Wald-style confidence intervals
for $\hat{\theta}^{\text{DE}}$.

The `medoutcon` package makes the estimation task quite simple, as only a single
call to the eponymous `medoutcon` function is required. As demonstrated below,
we need only feed in each component of the observed data $O = (W, A, Z, M, Y)$
(of which $W$ and $M$ can be multivariate), specify the effect type, and the
estimator. Additionally, for each nuisance parameter we may specify a separate
regression function --- in the examples below, we use the simpler Super Learner
ensemble constructed above for fitting each nuisance function, but this need not
be the case (i.e., different estimators may be used for each nuisance function).

First, we examine the one-step estimator of the interventional direct effect.
Recall that the one-step estimator is constructed by adding the mean of the EIF
(evaluated at initial estimates of the nuisance parameters) to the substitution
estimator. As noted above, this is done separately for each of the two contrasts
$(a' = 0, a^{\star} = 0)$ and $(a' = 1, a^{\star} = 0)$. Thus, the one-step
estimator of this direct effect is constructed by application of the delta
method to each of the one-step estimators (and EIFs) for these contrasts.

```{r de_os, message=FALSE, warning=FALSE}
# compute one-step estimate of the interventional direct effect
os_de <- medoutcon(W = example_data[, ..w_names],
                   A = example_data$A,
                   Z = example_data$Z,
                   M = example_data[, ..m_names],
                   Y = example_data$Y,
                   g_learners = sl_lrnr,
                   h_learners = sl_lrnr,
                   b_learners = sl_lrnr,
                   q_learners = sl_lrnr,
                   r_learners = sl_lrnr,
                   effect = "direct",
                   estimator = "onestep",
                   estimator_args = list(cv_folds = 2))
summary(os_de)
```

From the output of the summary method, we note that the one-step estimate of
the interventional direct effect $\hat{\theta}_{\text{os}}^{\text{DE}}$ is
`r round(os_de$theta, 3)`, with 95% confidence interval
[`r round(unname(confint(os_de, 0.95)[1]), 3)`,
`r round(unname(confint(os_de, 0.95)[3]), 3)`].

Next, let's compare the one-step estimate to the TML estimate. Analogous to the
case of the one-step estimator, the TML estimator can be evaluated via a single
call to the `medoutcon` function:

```{r de_tmle, message=FALSE, warning=FALSE}
# compute targeted minimum loss estimate of the interventional direct effect
tmle_de <- medoutcon(W = example_data[, ..w_names],
                     A = example_data$A,
                     Z = example_data$Z,
                     M = example_data[, ..m_names],
                     Y = example_data$Y,
                     g_learners = sl_lrnr,
                     h_learners = sl_lrnr,
                     b_learners = sl_lrnr,
                     q_learners = sl_lrnr,
                     r_learners = sl_lrnr,
                     effect = "direct",
                     estimator = "tmle",
                     estimator_args = list(cv_folds = 2, max_iter = 5))
summary(tmle_de)
```

From the output of the summary method, we note that the TML estimate of the
interventional direct effect $\hat{\theta}_{\text{tmle}}^{\text{DE}}$ is
`r round(tmle_de$theta, 3)`, with 95% confidence interval
[`r round(unname(confint(tmle_de, 0.95)[1]), 3)`,
`r round(unname(confint(tmle_de, 0.95)[3]), 3)`]. Here, we recall that the TML
estimator generally exhibits better finite-sample performance than the one-step
estimator [@vdl2011targeted; @vdl2018targeted], so the TML estimate is likely to
be more reliable in our modest sample size of $n =$ `r n_obs`.

## Estimating the indirect effect

Estimation of the interventional indirect effect proceeds similarly to the
strategy discussed above for the corresponding direct effect. An efficient
estimator can be computed as a contrast between the interventions $(a' = 1,
a^{\star} = 0)$ and $(a' = 1, a^{\star} = 1)$. Specifically, our efficient
estimators of the interventional indirect effect proceed by constructing
estimators $\hat{\theta}(a' = 1, a^{\star} = 0)$ and $\hat{\theta}(a' = 1,
a^{\star} = 1)$.  Then, application of the delta method yields an efficient
estimator of the indirect effect, that is, $\hat{\theta}^{\text{IE}} =
\hat{\theta}(a' = 1, a^{\star} = 0) - \hat{\theta}(a' = 1, a^{\star} = 1)$. The
same principle may be applied to the EIF estimates to derive variance estimates
and construct asymptotically correct Wald-style confidence intervals for
$\hat{\theta}^{\text{IE}}$.

Now, we examine the one-step estimator of the interventional indirect effect.
The one-step estimator is constructed by adding the mean of the EIF
(evaluated at initial estimates of the nuisance parameters) to the substitution
estimator. As noted above, this is done separately for each of the two contrasts
$(a' = 1, a^{\star} = 1)$ and $(a' = 1, a^{\star} = 0)$. Thus, the one-step
estimator of this indirect effect is constructed by application of the delta
method to each of the one-step estimators (and EIFs) for the contrasts.

```{r ie_os, message=FALSE, warning=FALSE}
# compute one-step estimate of the interventional indirect effect
os_ie <- medoutcon(W = example_data[, ..w_names],
                   A = example_data$A,
                   Z = example_data$Z,
                   M = example_data[, ..m_names],
                   Y = example_data$Y,
                   g_learners = sl_lrnr,
                   h_learners = sl_lrnr,
                   b_learners = sl_lrnr,
                   q_learners = sl_lrnr,
                   r_learners = sl_lrnr,
                   effect = "indirect",
                   estimator = "onestep")
summary(os_ie)
```

From the output of the summary method, we note that the one-step estimate of
the interventional indirect effect $\hat{\theta}_{\text{os}}^{\text{IE}}$ is
`r round(os_ie$theta, 3)`, with 95% confidence interval
[`r round(unname(confint(os_ie, 0.95)[1]), 3)`,
`r round(unname(confint(os_ie, 0.95)[3]), 3)`].

As before, let's compare the one-step estimate to the TML estimate. Analogous
to the case of the one-step estimator, the TML estimator can be evaluated via a
single call to the `medoutcon` function, as demonstrated below

```{r ie_tmle, message=FALSE, warning=FALSE}
# compute targeted minimum loss estimate of the interventional indirect effect
tmle_ie <- medoutcon(W = example_data[, ..w_names],
                     A = example_data$A,
                     Z = example_data$Z,
                     M = example_data[, ..m_names],
                     Y = example_data$Y,
                     g_learners = sl_lrnr,
                     h_learners = sl_lrnr,
                     b_learners = sl_lrnr,
                     q_learners = sl_lrnr,
                     r_learners = sl_lrnr,
                     effect = "indirect",
                     estimator = "tmle")
summary(tmle_ie)
```

From the output of the summary method, we note that the TML estimate of the
interventional indirect effect $\hat{\theta}_{\text{tmle}}^{\text{IE}}$ is
`r round(tmle_ie$theta, 3)`, with 95% confidence interval
[`r round(unname(confint(tmle_ie, 0.95)[1]), 3)`,
`r round(unname(confint(tmle_ie, 0.95)[3]), 3)`]. As before, the TML estimator
provides better finite-sample performance than the one-step estimator, so it may
be preferred in this example.

## References

